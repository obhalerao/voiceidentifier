{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import *\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "\n",
    "NUM_CLASSES = 5\n",
    "\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadfiles():\n",
    "    imgs = []\n",
    "    labels = []\n",
    "    for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "        for filename in filenames:\n",
    "            file = os.path.join(dirname, filename)\n",
    "            ext = filename.split('.')[1]\n",
    "            if ext == 'csv':\n",
    "                label = int(filename.split('_')[1])\n",
    "                imgs.append(np.loadtxt(open(file, \"rb\"), delimiter=\",\", skiprows=1))\n",
    "                labels.append(label)\n",
    "    return np.array(imgs), np.array(labels)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1326, 127, 100)\n"
     ]
    }
   ],
   "source": [
    "imgs, labels = loadfiles()\n",
    "print(imgs.shape)\n",
    "indices = np.array(list(range(imgs.shape[0])))\n",
    "np.random.shuffle(indices)\n",
    "test_imgs = imgs[indices[:300]]\n",
    "test_labels = labels[indices[:300]]\n",
    "train_imgs = imgs[indices[300:]]\n",
    "train_labels = labels[indices[300:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1.14507884e-01 1.18617021e-01 1.85893532e-02 ... 3.89510453e-01\n",
      "   5.37579775e-01 5.07201552e-01]\n",
      "  [7.78835788e-02 4.01288792e-02 8.20207410e-03 ... 1.32019907e-01\n",
      "   2.05461353e-01 1.62511840e-01]\n",
      "  [5.67057095e-02 2.92171538e-02 5.97179122e-03 ... 9.61214453e-02\n",
      "   1.49592921e-01 1.18322112e-01]\n",
      "  ...\n",
      "  [1.08691696e-02 2.88994727e-03 7.93149462e-04 ... 6.69958081e-06\n",
      "   1.10582932e-05 3.48337790e-06]\n",
      "  [5.55258989e-03 1.30137184e-03 1.33560286e-04 ... 6.01842294e-06\n",
      "   2.50285711e-06 2.21470168e-06]\n",
      "  [1.67713108e-04 9.05976849e-05 9.29192902e-06 ... 6.20881337e-07\n",
      "   3.21128283e-07 1.75063036e-07]]\n",
      "\n",
      " [[3.99187833e-01 3.88671868e-02 7.86780193e-02 ... 1.04915433e-01\n",
      "   6.08131997e-02 5.43594733e-02]\n",
      "  [2.00505719e-01 6.40276134e-01 6.74976110e-01 ... 3.53546068e-02\n",
      "   4.30643484e-02 1.54746734e-02]\n",
      "  [1.45984814e-01 4.66174185e-01 4.91438657e-01 ... 2.57410873e-02\n",
      "   3.13544199e-02 1.12668462e-02]\n",
      "  ...\n",
      "  [6.33579532e-07 2.31467723e-07 1.62229080e-05 ... 2.72951411e-06\n",
      "   1.81896405e-06 1.95381313e-06]\n",
      "  [3.83446513e-06 4.52567235e-07 1.51729646e-05 ... 3.11518011e-06\n",
      "   8.45762486e-07 7.60326714e-07]\n",
      "  [2.17339334e-06 6.87071804e-08 1.64166486e-05 ... 4.66108133e-07\n",
      "   1.61701301e-08 1.63285243e-08]]\n",
      "\n",
      " [[1.31405573e-02 1.88115146e-03 4.56915796e-02 ... 4.49774802e-01\n",
      "   2.96577990e-01 2.91013479e-01]\n",
      "  [2.62236416e-01 9.26469713e-02 1.50079399e-01 ... 1.64090383e+00\n",
      "   4.45349962e-01 1.00351584e+00]\n",
      "  [1.90929875e-01 6.74546883e-02 1.09270260e-01 ... 1.19471419e+00\n",
      "   3.24251741e-01 7.30642855e-01]\n",
      "  ...\n",
      "  [2.84006865e-06 1.72081138e-06 1.65984420e-06 ... 9.70882593e-06\n",
      "   1.87346268e-05 1.01123915e-05]\n",
      "  [9.05949321e-07 3.17364027e-07 1.43203835e-07 ... 5.49813285e-06\n",
      "   6.61560853e-06 2.29018247e-06]\n",
      "  [4.55794634e-07 2.70719518e-08 1.85520701e-08 ... 3.40563389e-07\n",
      "   4.11586370e-07 1.39630970e-07]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[1.36953807e+00 2.12736100e-01 1.19541816e-01 ... 2.41311356e-01\n",
      "   1.16187423e-01 3.85230258e-02]\n",
      "  [1.09449160e+00 1.04320288e-01 3.97087894e-02 ... 8.86114612e-02\n",
      "   5.59288040e-02 1.00538284e-02]\n",
      "  [7.96880722e-01 7.59538263e-02 2.89112944e-02 ... 6.45164996e-02\n",
      "   4.07208130e-02 7.32002221e-03]\n",
      "  ...\n",
      "  [4.46854137e-05 4.69183442e-06 1.81480391e-05 ... 2.75733487e-06\n",
      "   1.36458061e-06 5.80266146e-07]\n",
      "  [4.67434074e-05 2.38605867e-06 4.84226439e-06 ... 2.01427224e-06\n",
      "   7.47619310e-07 2.26342891e-07]\n",
      "  [5.02874500e-05 5.02329897e-07 1.85680562e-07 ... 7.33922363e-08\n",
      "   3.89618258e-08 4.65002969e-09]]\n",
      "\n",
      " [[9.01331529e-02 3.55127081e-02 1.21951595e-01 ... 1.42457575e-01\n",
      "   1.62466347e-01 2.11387947e-01]\n",
      "  [1.53048392e-02 5.24407774e-02 9.63246673e-02 ... 1.44801363e-01\n",
      "   2.92573810e-01 1.69779882e-01]\n",
      "  [1.11431936e-02 3.81812379e-02 7.01323524e-02 ... 1.05427414e-01\n",
      "   2.13018030e-01 1.23613849e-01]\n",
      "  ...\n",
      "  [5.91452317e-06 7.27118049e-06 1.33731683e-05 ... 1.88152426e-05\n",
      "   3.27870148e-05 2.48942633e-05]\n",
      "  [7.66787991e-07 2.50932158e-06 3.09391453e-06 ... 5.89650335e-06\n",
      "   8.17210639e-06 3.24219968e-06]\n",
      "  [1.33963351e-06 1.94185674e-07 1.07403736e-07 ... 8.05305547e-08\n",
      "   2.84555398e-07 1.16430101e-07]]\n",
      "\n",
      " [[2.27343179e-02 2.26308536e-02 2.39136145e-02 ... 6.48762798e-03\n",
      "   1.06401881e-03 5.48263267e-03]\n",
      "  [1.95189398e-02 8.39873590e-03 4.23257379e-03 ... 2.74501652e-01\n",
      "   1.84682816e-01 3.83671075e-01]\n",
      "  [1.42114088e-02 6.11497741e-03 3.08166514e-03 ... 1.99859977e-01\n",
      "   1.34464428e-01 2.79344380e-01]\n",
      "  ...\n",
      "  [1.52414548e-03 2.43888077e-04 1.21040910e-04 ... 7.75928947e-06\n",
      "   1.54946902e-05 5.81430177e-05]\n",
      "  [2.16776651e-04 6.57728742e-05 6.01538813e-05 ... 2.58107593e-06\n",
      "   5.26734266e-06 2.98923496e-05]\n",
      "  [2.23481220e-05 1.42787098e-06 5.53451491e-06 ... 4.18058988e-07\n",
      "   7.30681847e-07 1.27535523e-05]]]\n"
     ]
    }
   ],
   "source": [
    "print(test_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_arr = [[] for i in range(NUM_CLASSES)]\n",
    "for idx, i in enumerate(train_labels):\n",
    "    train_arr[i].append(idx)\n",
    "test_arr = [[] for i in range(NUM_CLASSES)]\n",
    "for idx, i in enumerate(test_labels):\n",
    "    test_arr[i].append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size, s=\"train\"):\n",
    "    global NUM_CLASSES, train_imgs, test_imgs, train_labels, test_labels, train_arr, test_arr\n",
    "    if s == \"train\":\n",
    "        imgs = train_imgs\n",
    "        labels = train_labels\n",
    "        arr = train_arr\n",
    "    else:\n",
    "        imgs = test_imgs\n",
    "        labels = test_labels\n",
    "        arr = test_arr\n",
    "        \n",
    "    \n",
    "    n_classes = NUM_CLASSES\n",
    "    n_examples, w, h = imgs.shape\n",
    "    pairs = [np.zeros((batch_size, w, h, 1)) for i in range(2)]\n",
    "    targets = np.zeros((batch_size,))\n",
    "    \n",
    "    targets[batch_size//2:] = 1\n",
    "    \n",
    "    categories = np.random.randint(0, n_classes, batch_size)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        category = categories[i]\n",
    "        idx_1 = random.choice(arr[category])\n",
    "        pairs[0][i,:,:,:] = imgs[idx_1].reshape(w, h, 1)\n",
    "        if i >= batch_size // 2:\n",
    "            category_2 = category  \n",
    "        else: \n",
    "            category_2 = (category + np.random.randint(1,n_classes)) % n_classes\n",
    "        idx_2 = random.choice(arr[category_2])\n",
    "        \n",
    "        pairs[1][i,:,:,:] = imgs[idx_2].reshape(w, h, 1)\n",
    "    \n",
    "    return pairs, targets\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(batch_size, s=\"train\"):\n",
    "    while True:\n",
    "        pairs, targets = get_batch(batch_size, s)\n",
    "        yield (pairs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_siamese_model(input_shape):\n",
    "    \"\"\"\n",
    "        Model architecture\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the tensors for the two input images\n",
    "    left_input = Input(input_shape)\n",
    "    right_input = Input(input_shape)\n",
    "    \n",
    "    # Convolutional Neural Network\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (10,10), activation='relu', input_shape=input_shape,\n",
    "                   kernel_initializer='random_normal', kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(128, (7,7), activation='relu',\n",
    "                     kernel_initializer='random_normal',\n",
    "                     bias_initializer='random_normal', kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(128, (4,4), activation='relu', kernel_initializer='random_normal',\n",
    "                     bias_initializer='random_normal', kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(256, (4,4), activation='relu', kernel_initializer='random_normal',\n",
    "                     bias_initializer='random_normal', kernel_regularizer=l2(2e-4)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='sigmoid',\n",
    "                   kernel_regularizer=l2(1e-3),\n",
    "                   kernel_initializer='random_normal',bias_initializer='random_normal'))\n",
    "    \n",
    "    # Generate the encodings (feature vectors) for the two images\n",
    "    encoded_l = model(left_input)\n",
    "    encoded_r = model(right_input)\n",
    "    \n",
    "    # Add a customized layer to compute the absolute difference between the encodings\n",
    "    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "    \n",
    "    # Add a dense layer with a sigmoid unit to generate the similarity score\n",
    "    prediction = Dense(1,activation='sigmoid',bias_initializer='random_normal')(L1_distance)\n",
    "    \n",
    "    # Connect the inputs with the outputs\n",
    "    siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "    \n",
    "    # return the model\n",
    "    return siamese_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "100/100 [==============================] - 11s 108ms/step - loss: 99.6061\n",
      "Epoch 2/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 87.5587\n",
      "Epoch 3/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 77.2512\n",
      "Epoch 4/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 68.3891\n",
      "Epoch 5/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 60.7178\n",
      "Epoch 6/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 54.0565\n",
      "Epoch 7/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 48.2529\n",
      "Epoch 8/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 43.1871\n",
      "Epoch 9/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 38.7755\n",
      "Epoch 10/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 34.8996\n",
      "Epoch 11/300\n",
      "100/100 [==============================] - 6s 59ms/step - loss: 31.5141\n",
      "Epoch 12/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 28.5345\n",
      "Epoch 13/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 25.8962\n",
      "Epoch 14/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 23.5829\n",
      "Epoch 15/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 21.5324\n",
      "Epoch 16/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 19.7294\n",
      "Epoch 17/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 18.1205\n",
      "Epoch 18/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 16.7134\n",
      "Epoch 19/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 15.4479\n",
      "Epoch 20/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 14.3066\n",
      "Epoch 21/300\n",
      "100/100 [==============================] - 6s 59ms/step - loss: 13.3021\n",
      "Epoch 22/300\n",
      "100/100 [==============================] - 6s 59ms/step - loss: 12.3838\n",
      "Epoch 23/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 11.5746\n",
      "Epoch 24/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 10.8371\n",
      "Epoch 25/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 10.1669\n",
      "Epoch 26/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 9.5597\n",
      "Epoch 27/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 9.0085\n",
      "Epoch 28/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 8.5110\n",
      "Epoch 29/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 8.0581\n",
      "Epoch 30/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 7.6284\n",
      "Epoch 31/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 7.2354\n",
      "Epoch 32/300\n",
      "100/100 [==============================] - 6s 59ms/step - loss: 6.8725\n",
      "Epoch 33/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 6.5330\n",
      "Epoch 34/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 6.2235\n",
      "Epoch 35/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 5.9281\n",
      "Epoch 36/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 5.6653\n",
      "Epoch 37/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 5.3883\n",
      "Epoch 38/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 5.1283\n",
      "Epoch 39/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 4.9164\n",
      "Epoch 40/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 4.6925\n",
      "Epoch 41/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 4.4821\n",
      "Epoch 42/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 4.2834\n",
      "Epoch 43/300\n",
      "100/100 [==============================] - 6s 59ms/step - loss: 4.0928\n",
      "Epoch 44/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 3.9128\n",
      "Epoch 45/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 3.7353\n",
      "Epoch 46/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 3.5750\n",
      "Epoch 47/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 3.4236\n",
      "Epoch 48/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 3.2702\n",
      "Epoch 49/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 3.1331\n",
      "Epoch 50/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 2.9963\n",
      "Epoch 51/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 2.8715\n",
      "Epoch 52/300\n",
      "100/100 [==============================] - 6s 59ms/step - loss: 2.7617\n",
      "Epoch 53/300\n",
      "100/100 [==============================] - 6s 59ms/step - loss: 2.6357\n",
      "Epoch 54/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 2.5240\n",
      "Epoch 55/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.8679\n",
      "Epoch 82/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.8380\n",
      "Epoch 83/300\n",
      "100/100 [==============================] - 6s 59ms/step - loss: 0.8086\n",
      "Epoch 84/300\n",
      "100/100 [==============================] - 6s 59ms/step - loss: 0.7785\n",
      "Epoch 85/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 0.7518\n",
      "Epoch 86/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 0.7278\n",
      "Epoch 87/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 0.7016\n",
      "Epoch 88/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 0.6802\n",
      "Epoch 89/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.6631\n",
      "Epoch 90/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.6389\n",
      "Epoch 91/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.6214\n",
      "Epoch 92/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.6390\n",
      "Epoch 93/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 0.6127\n",
      "Epoch 94/300\n",
      "100/100 [==============================] - 6s 59ms/step - loss: 0.5829\n",
      "Epoch 95/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 0.5638\n",
      "Epoch 96/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.6393\n",
      "Epoch 97/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.6006\n",
      "Epoch 98/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 0.5593\n",
      "Epoch 99/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.5406\n",
      "Epoch 100/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 0.5231\n",
      "Epoch 101/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.5119\n",
      "Epoch 102/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 0.5000\n",
      "Epoch 103/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.4886\n",
      "Epoch 104/300\n",
      "100/100 [==============================] - 6s 59ms/step - loss: 0.4774\n",
      "Epoch 105/300\n",
      "100/100 [==============================] - 6s 59ms/step - loss: 0.4655\n",
      "Epoch 106/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 0.4577\n",
      "Epoch 107/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.4903\n",
      "Epoch 108/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.4700\n",
      "Epoch 109/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.4498\n",
      "Epoch 110/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 0.4334\n",
      "Epoch 111/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 0.4231\n",
      "Epoch 112/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.4160\n",
      "Epoch 113/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.4058\n",
      "Epoch 114/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.4009\n",
      "Epoch 115/300\n",
      "100/100 [==============================] - 6s 59ms/step - loss: 0.3916\n",
      "Epoch 116/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 0.3834\n",
      "Epoch 117/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 0.3765\n",
      "Epoch 118/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.3689\n",
      "Epoch 119/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.3609\n",
      "Epoch 120/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.5336\n",
      "Epoch 121/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 0.4407\n",
      "Epoch 122/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.3955\n",
      "Epoch 123/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 0.3814\n",
      "Epoch 124/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.3681\n",
      "Epoch 125/300\n",
      "100/100 [==============================] - 6s 59ms/step - loss: 0.3600\n",
      "Epoch 126/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 0.3524\n",
      "Epoch 127/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 0.3955\n",
      "Epoch 128/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.3599\n",
      "Epoch 129/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 0.3444\n",
      "Epoch 130/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.3366\n",
      "Epoch 131/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 0.3310\n",
      "Epoch 132/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 0.3284\n",
      "Epoch 133/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.3226\n",
      "Epoch 134/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.3182\n",
      "Epoch 135/300\n",
      "100/100 [==============================] - 6s 59ms/step - loss: 0.3129\n",
      "Epoch 136/300\n",
      "100/100 [==============================] - 6s 59ms/step - loss: 0.3100\n",
      "Epoch 137/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 0.3046\n",
      "Epoch 138/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.3055\n",
      "Epoch 139/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.3025\n",
      "Epoch 140/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 0.3040\n",
      "Epoch 141/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 0.2937\n",
      "Epoch 142/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.2920\n",
      "Epoch 143/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.2881\n",
      "Epoch 144/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.2814\n",
      "Epoch 145/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.2783\n",
      "Epoch 146/300\n",
      "100/100 [==============================] - 6s 59ms/step - loss: 0.3604\n",
      "Epoch 147/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 0.2989\n",
      "Epoch 148/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 0.2873\n",
      "Epoch 149/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.2847\n",
      "Epoch 150/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.2774\n",
      "Epoch 151/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.2745\n",
      "Epoch 152/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.2710\n",
      "Epoch 153/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.2700\n",
      "Epoch 154/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.2669\n",
      "Epoch 155/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.2604\n",
      "Epoch 156/300\n",
      "  3/100 [..............................] - ETA: 5s - loss: 0.2569"
     ]
    }
   ],
   "source": [
    "model = get_siamese_model(tuple(list(train_imgs[0].shape)+[1]))\n",
    "optimizer = Adam(lr = 0.00006)\n",
    "model.compile(loss=\"binary_crossentropy\",optimizer=optimizer)\n",
    "    \n",
    "model.fit(generate(32), epochs=300, steps_per_epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('epoch300-weights-new.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_oneshot_task(N, s=\"val\"):\n",
    "    global NUM_CLASSES, train_imgs, test_imgs, train_labels, test_labels, train_arr, test_arr\n",
    "    if s == \"train\":\n",
    "        imgs = train_imgs\n",
    "        labels = train_labels\n",
    "        arr = train_arr\n",
    "    else:\n",
    "        imgs = test_imgs\n",
    "        labels = test_labels\n",
    "        arr = test_arr\n",
    "        \n",
    "    n_classes = NUM_CLASSES\n",
    "    n_examples, w, h = imgs.shape\n",
    "    \n",
    "    classes = list(range(NUM_CLASSES))\n",
    "\n",
    "    random.shuffle(classes)\n",
    "    \n",
    "    true_class = classes[0]\n",
    "    \n",
    "    idx1, idx2 = np.random.choice(arr[true_class], size=(2,))\n",
    "    \n",
    "    test_image = np.asarray([imgs[idx1,:,:]]*N).reshape(N, w, h, 1)\n",
    "    \n",
    "    ssidxs = [idx2]\n",
    "    for i in range(1, NUM_CLASSES):\n",
    "        ssidxs.append(np.random.choice(arr[classes[i]]))\n",
    "    \n",
    "    support_set = imgs[ssidxs,:,:]\n",
    "    \n",
    "    support_set = support_set.reshape(N, w, h, 1)\n",
    "    \n",
    "    targets = np.zeros((N,))\n",
    "    targets[0] = 1\n",
    "    \n",
    "    test_image, support_set, targets = shuffle(test_image, support_set, targets)\n",
    "    \n",
    "    \n",
    "    pairs = [test_image, support_set]\n",
    "    \n",
    "    return pairs, targets, classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_oneshot(model, N, k, s = \"val\", verbose = 0):\n",
    "    global NUM_CLASSES\n",
    "    n_correct = 0\n",
    "    total = [0 for i in range(NUM_CLASSES)]\n",
    "    correct = [0 for i in range(NUM_CLASSES)]\n",
    "    if verbose:\n",
    "        print(\"Evaluating model on {} random {} way one-shot learning tasks ... \\n\".format(k,N))\n",
    "    for i in range(k):\n",
    "        inputs, targets, classes = make_oneshot_task(N,s)\n",
    "        total[classes[np.argwhere(targets == 1)[0,0]]]+=1\n",
    "        probs = model.predict(inputs)\n",
    "        if np.argmax(probs) == np.argmax(targets):\n",
    "            n_correct+=1\n",
    "            correct[classes[np.argwhere(targets == 1)[0,0]]]+=1\n",
    "    percent_correct = (100.0 * n_correct / k)\n",
    "    if verbose:\n",
    "        print(\"Got an average of {}% {} way one-shot learning accuracy \\n\".format(percent_correct,N))\n",
    "    percentages = [(100.0*a)/b for a, b in zip(correct, total)]\n",
    "    return percentages, percent_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on 1000 random 5 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 67.9% 5 way one-shot learning accuracy \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([64.79591836734694,\n",
       "  66.5158371040724,\n",
       "  70.0,\n",
       "  69.79166666666667,\n",
       "  68.50828729281768],\n",
       " 67.9)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_oneshot(model, 5, 1000, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
