{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import *\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "\n",
    "NUM_CLASSES = 5\n",
    "\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadfiles():\n",
    "    imgs = []\n",
    "    labels = []\n",
    "    for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "        for filename in filenames:\n",
    "            file = os.path.join(dirname, filename)\n",
    "            ext = filename.split('.')[1]\n",
    "            if ext == 'csv':\n",
    "                label = int(filename.split('_')[1])\n",
    "                imgs.append(np.loadtxt(open(file, \"rb\"), delimiter=\",\", skiprows=1))\n",
    "                labels.append(label)\n",
    "    return np.array(imgs), np.array(labels)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = loadfiles()\n",
    "indices = np.array(list(range(imgs.shape[0])))\n",
    "np.random.shuffle(indices)\n",
    "test_imgs = imgs[indices[:1000]]\n",
    "test_labels = labels[indices[:1000]]\n",
    "train_imgs = imgs[indices[1000:]]\n",
    "train_labels = labels[indices[1000:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_arr = [[] for i in range(NUM_CLASSES)]\n",
    "for idx, i in enumerate(train_labels):\n",
    "    train_arr[i].append(idx)\n",
    "test_arr = [[] for i in range(NUM_CLASSES)]\n",
    "for idx, i in enumerate(test_labels):\n",
    "    test_arr[i].append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size, s=\"train\"):\n",
    "    global NUM_CLASSES, train_imgs, test_imgs, train_labels, test_labels, train_arr, test_arr\n",
    "    if s == \"train\":\n",
    "        imgs = train_imgs\n",
    "        labels = train_labels\n",
    "        arr = train_arr\n",
    "    else:\n",
    "        imgs = test_imgs\n",
    "        labels = test_labels\n",
    "        arr = test_arr\n",
    "        \n",
    "    \n",
    "    n_classes = NUM_CLASSES\n",
    "    n_examples, w, h = imgs.shape\n",
    "    pairs = [np.zeros((batch_size, w, h, 1)) for i in range(2)]\n",
    "    targets = np.zeros((batch_size,))\n",
    "    \n",
    "    targets[batch_size//2:] = 1\n",
    "    \n",
    "    categories = np.random.randint(0, n_classes, batch_size)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        category = categories[i]\n",
    "        idx_1 = random.choice(arr[category])\n",
    "        pairs[0][i,:,:,:] = imgs[idx_1].reshape(w, h, 1)\n",
    "        if i >= batch_size // 2:\n",
    "            category_2 = category  \n",
    "        else: \n",
    "            category_2 = (category + np.random.randint(1,n_classes)) % n_classes\n",
    "        idx_2 = random.choice(arr[category_2])\n",
    "        \n",
    "        pairs[1][i,:,:,:] = imgs[idx_2].reshape(w, h, 1)\n",
    "    \n",
    "    return pairs, targets\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(batch_size, s=\"train\"):\n",
    "    while True:\n",
    "        pairs, targets = get_batch(batch_size, s)\n",
    "        yield (pairs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_siamese_model(input_shape):\n",
    "    \"\"\"\n",
    "        Model architecture\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the tensors for the two input images\n",
    "    left_input = Input(input_shape)\n",
    "    right_input = Input(input_shape)\n",
    "    \n",
    "    # Convolutional Neural Network\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (10,10), activation='relu', input_shape=input_shape,\n",
    "                   kernel_initializer='random_normal', kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(128, (7,7), activation='relu',\n",
    "                     kernel_initializer='random_normal',\n",
    "                     bias_initializer='random_normal', kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(128, (4,4), activation='relu', kernel_initializer='random_normal',\n",
    "                     bias_initializer='random_normal', kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(256, (4,4), activation='relu', kernel_initializer='random_normal',\n",
    "                     bias_initializer='random_normal', kernel_regularizer=l2(2e-4)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='sigmoid',\n",
    "                   kernel_regularizer=l2(1e-3),\n",
    "                   kernel_initializer='random_normal',bias_initializer='random_normal'))\n",
    "    \n",
    "    # Generate the encodings (feature vectors) for the two images\n",
    "    encoded_l = model(left_input)\n",
    "    encoded_r = model(right_input)\n",
    "    \n",
    "    # Add a customized layer to compute the absolute difference between the encodings\n",
    "    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "    \n",
    "    # Add a dense layer with a sigmoid unit to generate the similarity score\n",
    "    prediction = Dense(1,activation='sigmoid',bias_initializer='random_normal')(L1_distance)\n",
    "    \n",
    "    # Connect the inputs with the outputs\n",
    "    siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "    \n",
    "    # return the model\n",
    "    return siamese_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_siamese_model(tuple(list(train_imgs[0].shape)+[1]))\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        if(filename == 'epoch150-weights.h5'):\n",
    "            file = os.path.join(dirname, filename)\n",
    "            model.load_weights(file)\n",
    "\n",
    "optimizer = Adam(lr = 0.00006)\n",
    "model.compile(loss=\"binary_crossentropy\",optimizer=optimizer)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_oneshot_task(N, s=\"val\"):\n",
    "    global NUM_CLASSES, train_imgs, test_imgs, train_labels, test_labels, train_arr, test_arr\n",
    "    if s == \"train\":\n",
    "        imgs = train_imgs\n",
    "        labels = train_labels\n",
    "        arr = train_arr\n",
    "    else:\n",
    "        imgs = test_imgs\n",
    "        labels = test_labels\n",
    "        arr = test_arr\n",
    "        \n",
    "    n_classes = NUM_CLASSES\n",
    "    n_examples, w, h = imgs.shape\n",
    "    \n",
    "    classes = list(range(NUM_CLASSES))\n",
    "\n",
    "    random.shuffle(classes)\n",
    "    \n",
    "    true_class = classes[0]\n",
    "    \n",
    "    idx1, idx2 = np.random.choice(arr[true_class], size=(2,))\n",
    "    \n",
    "    test_image = np.asarray([imgs[idx1,:,:]]*N).reshape(N, w, h, 1)\n",
    "    \n",
    "    ssidxs = [idx2]\n",
    "    for i in range(1, NUM_CLASSES):\n",
    "        ssidxs.append(np.random.choice(arr[classes[i]]))\n",
    "    \n",
    "    support_set = imgs[ssidxs,:,:]\n",
    "    \n",
    "    support_set = support_set.reshape(N, w, h, 1)\n",
    "    \n",
    "    targets = np.zeros((N,))\n",
    "    targets[0] = 1\n",
    "    \n",
    "    test_image, support_set, targets = shuffle(test_image, support_set, targets)\n",
    "    \n",
    "    \n",
    "    pairs = [test_image, support_set]\n",
    "    \n",
    "    return pairs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_oneshot(model, N, k, s = \"val\", verbose = 0):\n",
    "    n_correct = 0\n",
    "    if verbose:\n",
    "        print(\"Evaluating model on {} random {} way one-shot learning tasks ... \\n\".format(k,N))\n",
    "    for i in range(k):\n",
    "        inputs, targets = make_oneshot_task(N,s)\n",
    "        probs = model.predict(inputs)\n",
    "        if np.argmax(probs) == np.argmax(targets):\n",
    "            n_correct+=1\n",
    "    percent_correct = (100.0 * n_correct / k)\n",
    "    if verbose:\n",
    "        print(\"Got an average of {}% {} way one-shot learning accuracy \\n\".format(percent_correct,N))\n",
    "    return percent_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on 1000 random 5 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 94.4% 5 way one-shot learning accuracy \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "94.4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_oneshot(model, 5, 1000, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
