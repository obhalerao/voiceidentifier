{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import *\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadfiles():\n",
    "    imgs = []\n",
    "    labels = []\n",
    "    for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "        for filename in filenames:\n",
    "            file = os.path.join(dirname, filename)\n",
    "            ext = filename.split('.')[1]\n",
    "            if ext == 'csv':\n",
    "                label = int(filename.split('_')[1])\n",
    "                imgs.append(np.loadtxt(open(file, \"rb\"), delimiter=\",\", skiprows=1))\n",
    "                labels.append(label)\n",
    "    return np.array(imgs), np.array(labels)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3588, 127, 100)\n"
     ]
    }
   ],
   "source": [
    "imgs, labels = loadfiles()\n",
    "print(imgs.shape)\n",
    "indices = np.array(list(range(imgs.shape[0])))\n",
    "np.random.shuffle(indices)\n",
    "test_imgs = imgs[indices[:200]]\n",
    "test_labels = labels[indices[:200]]\n",
    "train_imgs = imgs[indices[200:]]\n",
    "train_labels = labels[indices[200:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_arr = [[] for i in range(NUM_CLASSES)]\n",
    "for idx, i in enumerate(train_labels):\n",
    "    train_arr[i].append(idx)\n",
    "test_arr = [[] for i in range(NUM_CLASSES)]\n",
    "for idx, i in enumerate(test_labels):\n",
    "    test_arr[i].append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size, s=\"train\"):\n",
    "    global NUM_CLASSES, train_imgs, test_imgs, train_labels, test_labels, train_arr, test_arr\n",
    "    if s == \"train\":\n",
    "        imgs = train_imgs\n",
    "        labels = train_labels\n",
    "        arr = train_arr\n",
    "    else:\n",
    "        imgs = test_imgs\n",
    "        labels = test_labels\n",
    "        arr = test_arr\n",
    "        \n",
    "    \n",
    "    n_classes = NUM_CLASSES\n",
    "    n_examples, w, h = imgs.shape\n",
    "    pairs = [np.zeros((batch_size, w, h, 1)) for i in range(2)]\n",
    "    targets = np.zeros((batch_size,))\n",
    "    \n",
    "    targets[batch_size//2:] = 1\n",
    "    \n",
    "    categories = np.random.randint(0, n_classes, batch_size)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        category = categories[i]\n",
    "        idx_1 = random.choice(arr[category])\n",
    "        pairs[0][i,:,:,:] = imgs[idx_1].reshape(w, h, 1)\n",
    "        if i >= batch_size // 2:\n",
    "            category_2 = category  \n",
    "        else: \n",
    "            category_2 = (category + np.random.randint(1,n_classes)) % n_classes\n",
    "        idx_2 = random.choice(arr[category_2])\n",
    "        \n",
    "        pairs[1][i,:,:,:] = imgs[idx_2].reshape(w, h, 1)\n",
    "    \n",
    "    return pairs, targets\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(batch_size, s=\"train\"):\n",
    "    while True:\n",
    "        pairs, targets = get_batch(batch_size, s)\n",
    "        yield (pairs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_siamese_model(input_shape):\n",
    "    \"\"\"\n",
    "        Model architecture\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the tensors for the two input images\n",
    "    left_input = Input(input_shape)\n",
    "    right_input = Input(input_shape)\n",
    "    \n",
    "    # Convolutional Neural Network\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (10,10), activation='relu', input_shape=input_shape,\n",
    "                   kernel_initializer='random_normal', kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(128, (7,7), activation='relu',\n",
    "                     kernel_initializer='random_normal',\n",
    "                     bias_initializer='random_normal', kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(128, (4,4), activation='relu', kernel_initializer='random_normal',\n",
    "                     bias_initializer='random_normal', kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(256, (4,4), activation='relu', kernel_initializer='random_normal',\n",
    "                     bias_initializer='random_normal', kernel_regularizer=l2(2e-4)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='sigmoid',\n",
    "                   kernel_regularizer=l2(1e-3),\n",
    "                   kernel_initializer='random_normal',bias_initializer='random_normal'))\n",
    "    \n",
    "    # Generate the encodings (feature vectors) for the two images\n",
    "    encoded_l = model(left_input)\n",
    "    encoded_r = model(right_input)\n",
    "    \n",
    "    # Add a customized layer to compute the absolute difference between the encodings\n",
    "    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "    \n",
    "    # Add a dense layer with a sigmoid unit to generate the similarity score\n",
    "    prediction = Dense(1,activation='sigmoid',bias_initializer='random_normal')(L1_distance)\n",
    "    \n",
    "    # Connect the inputs with the outputs\n",
    "    siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "    \n",
    "    # return the model\n",
    "    return siamese_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "100/100 [==============================] - 10s 96ms/step - loss: 100.5940\n",
      "Epoch 2/300\n",
      "100/100 [==============================] - 6s 55ms/step - loss: 90.8410\n",
      "Epoch 3/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 82.4627\n",
      "Epoch 4/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 75.1284\n",
      "Epoch 5/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 68.6654\n",
      "Epoch 6/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 62.9266\n",
      "Epoch 7/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 57.8007\n",
      "Epoch 8/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 53.2469\n",
      "Epoch 9/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 49.1715\n",
      "Epoch 10/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 45.5228\n",
      "Epoch 11/300\n",
      "100/100 [==============================] - 6s 55ms/step - loss: 42.2400\n",
      "Epoch 12/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 39.2944\n",
      "Epoch 13/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 36.6482\n",
      "Epoch 14/300\n",
      "100/100 [==============================] - 6s 55ms/step - loss: 34.2518\n",
      "Epoch 15/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 32.0868\n",
      "Epoch 16/300\n",
      "100/100 [==============================] - 6s 55ms/step - loss: 30.1113\n",
      "Epoch 17/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 28.3377\n",
      "Epoch 18/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 26.6819\n",
      "Epoch 19/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 25.1944\n",
      "Epoch 20/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 23.8237\n",
      "Epoch 21/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 22.5680\n",
      "Epoch 22/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 21.4112\n",
      "Epoch 23/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 20.3317\n",
      "Epoch 24/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 19.3315\n",
      "Epoch 25/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 18.4142\n",
      "Epoch 26/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 17.5319\n",
      "Epoch 27/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 16.6995\n",
      "Epoch 28/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 15.9169\n",
      "Epoch 29/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 15.2093\n",
      "Epoch 30/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 14.5198\n",
      "Epoch 31/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 13.8548\n",
      "Epoch 32/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 13.2400\n",
      "Epoch 33/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 12.6618\n",
      "Epoch 34/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 12.0936\n",
      "Epoch 35/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 11.5597\n",
      "Epoch 36/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 11.0529\n",
      "Epoch 37/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 10.5749\n",
      "Epoch 38/300\n",
      "100/100 [==============================] - 6s 55ms/step - loss: 10.1103\n",
      "Epoch 39/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 9.6733\n",
      "Epoch 40/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 9.2492\n",
      "Epoch 41/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 8.8382\n",
      "Epoch 42/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 8.4440\n",
      "Epoch 43/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 8.0689\n",
      "Epoch 44/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 7.7099\n",
      "Epoch 45/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 7.3589\n",
      "Epoch 46/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 7.0289\n",
      "Epoch 47/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 6.7172\n",
      "Epoch 48/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 6.4116\n",
      "Epoch 49/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 6.1130\n",
      "Epoch 50/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 5.8301\n",
      "Epoch 51/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 5.5514\n",
      "Epoch 52/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 5.2950\n",
      "Epoch 53/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 5.0482\n",
      "Epoch 54/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 4.8156\n",
      "Epoch 55/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 1.1678\n",
      "Epoch 85/300\n",
      "100/100 [==============================] - 6s 55ms/step - loss: 1.1174\n",
      "Epoch 86/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 1.0850\n",
      "Epoch 87/300\n",
      "100/100 [==============================] - 6s 55ms/step - loss: 1.0486\n",
      "Epoch 88/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 1.0079\n",
      "Epoch 89/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.9805\n",
      "Epoch 90/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.9357\n",
      "Epoch 91/300\n",
      "100/100 [==============================] - 6s 55ms/step - loss: 0.8973\n",
      "Epoch 92/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.8664\n",
      "Epoch 93/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.8579\n",
      "Epoch 94/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.8200\n",
      "Epoch 95/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.7921\n",
      "Epoch 96/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.7697\n",
      "Epoch 97/300\n",
      "100/100 [==============================] - 6s 55ms/step - loss: 0.7437\n",
      "Epoch 98/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.7287\n",
      "Epoch 99/300\n",
      "100/100 [==============================] - 6s 55ms/step - loss: 0.6964\n",
      "Epoch 100/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.6851\n",
      "Epoch 101/300\n",
      "100/100 [==============================] - 6s 55ms/step - loss: 0.6661\n",
      "Epoch 102/300\n",
      "100/100 [==============================] - 6s 55ms/step - loss: 0.6407\n",
      "Epoch 103/300\n",
      "100/100 [==============================] - 6s 55ms/step - loss: 0.6301\n",
      "Epoch 104/300\n",
      "100/100 [==============================] - 6s 55ms/step - loss: 0.6128\n",
      "Epoch 105/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.5966\n",
      "Epoch 106/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.5730\n",
      "Epoch 107/300\n",
      "100/100 [==============================] - 6s 55ms/step - loss: 0.5638\n",
      "Epoch 108/300\n",
      "100/100 [==============================] - 6s 55ms/step - loss: 0.5524\n",
      "Epoch 109/300\n",
      "100/100 [==============================] - 6s 55ms/step - loss: 0.5341\n",
      "Epoch 110/300\n",
      "100/100 [==============================] - 6s 55ms/step - loss: 0.5182\n",
      "Epoch 111/300\n",
      "100/100 [==============================] - 6s 55ms/step - loss: 0.5599\n",
      "Epoch 112/300\n",
      "100/100 [==============================] - 6s 55ms/step - loss: 0.5169\n",
      "Epoch 113/300\n",
      "100/100 [==============================] - 6s 55ms/step - loss: 0.4930\n",
      "Epoch 114/300\n",
      "100/100 [==============================] - 6s 55ms/step - loss: 0.4901\n",
      "Epoch 115/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.4761\n",
      "Epoch 116/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.4694\n",
      "Epoch 117/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.4653\n",
      "Epoch 118/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.4510\n",
      "Epoch 119/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.4396\n",
      "Epoch 120/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.4369\n",
      "Epoch 121/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.4246\n",
      "Epoch 122/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.4122\n",
      "Epoch 123/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.4106\n",
      "Epoch 124/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.4050\n",
      "Epoch 125/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.3964\n",
      "Epoch 126/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.3883\n",
      "Epoch 127/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.3794\n",
      "Epoch 128/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.4250\n",
      "Epoch 129/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.3756\n",
      "Epoch 130/300\n",
      "100/100 [==============================] - 6s 55ms/step - loss: 0.3687\n",
      "Epoch 131/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.3619\n",
      "Epoch 132/300\n",
      "100/100 [==============================] - 6s 55ms/step - loss: 0.3566\n",
      "Epoch 133/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.3596\n",
      "Epoch 134/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.3492\n",
      "Epoch 135/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.3431\n",
      "Epoch 136/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.3376\n",
      "Epoch 137/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.3378\n",
      "Epoch 138/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.3315\n",
      "Epoch 139/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.3260\n",
      "Epoch 140/300\n",
      "100/100 [==============================] - 6s 55ms/step - loss: 0.3169\n",
      "Epoch 141/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.3153\n",
      "Epoch 142/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.3079\n",
      "Epoch 143/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.3053\n",
      "Epoch 144/300\n",
      "100/100 [==============================] - 6s 55ms/step - loss: 0.3028\n",
      "Epoch 145/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.2958\n",
      "Epoch 146/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.2992\n",
      "Epoch 147/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.3451\n",
      "Epoch 148/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.2977\n",
      "Epoch 149/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.2924\n",
      "Epoch 150/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.2945\n",
      "Epoch 151/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.2954\n",
      "Epoch 152/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.2839\n",
      "Epoch 153/300\n",
      " 91/100 [==========================>...] - ETA: 0s - loss: 0.2810"
     ]
    }
   ],
   "source": [
    "model = get_siamese_model(tuple(list(train_imgs[0].shape)+[1]))\n",
    "optimizer = Adam(lr = 0.00006)\n",
    "model.compile(loss=\"binary_crossentropy\",optimizer=optimizer)\n",
    "    \n",
    "model.fit(generate(32), epochs=300, steps_per_epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_oneshot_task(N, s=\"val\"):\n",
    "    global NUM_CLASSES, train_imgs, test_imgs, train_labels, test_labels, train_arr, test_arr\n",
    "    if s == \"train\":\n",
    "        imgs = train_imgs\n",
    "        labels = train_labels\n",
    "        arr = train_arr\n",
    "    else:\n",
    "        imgs = test_imgs\n",
    "        labels = test_labels\n",
    "        arr = test_arr\n",
    "        \n",
    "    n_classes = NUM_CLASSES\n",
    "    n_examples, w, h = imgs.shape\n",
    "    \n",
    "    classes = list(range(NUM_CLASSES))\n",
    "\n",
    "    random.shuffle(classes)\n",
    "    \n",
    "    true_class = classes[0]\n",
    "    \n",
    "    idx1, idx2 = np.random.choice(arr[true_class], size=(2,))\n",
    "    \n",
    "    test_image = np.asarray([imgs[idx1,:,:]]*N).reshape(N, w, h, 1)\n",
    "    \n",
    "    ssidxs = [idx2]\n",
    "    for i in range(1, NUM_CLASSES):\n",
    "        ssidxs.append(np.random.choice(arr[classes[i]]))\n",
    "    \n",
    "    support_set = imgs[ssidxs,:,:]\n",
    "    \n",
    "    support_set = support_set.reshape(N, w, h, 1)\n",
    "    \n",
    "    targets = np.zeros((N,))\n",
    "    targets[0] = 1\n",
    "    \n",
    "    test_image, support_set, targets = shuffle(test_image, support_set, targets)\n",
    "    \n",
    "    \n",
    "    pairs = [test_image, support_set]\n",
    "    \n",
    "    return pairs, targets, classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_oneshot(model, N, k, s = \"val\", verbose = 0):\n",
    "    global NUM_CLASSES\n",
    "    n_correct = 0\n",
    "    total = [0 for i in range(NUM_CLASSES)]\n",
    "    correct = [0 for i in range(NUM_CLASSES)]\n",
    "    if verbose:\n",
    "        print(\"Evaluating model on {} random {} way one-shot learning tasks ... \\n\".format(k,N))\n",
    "    for i in range(k):\n",
    "        inputs, targets, classes = make_oneshot_task(N,s)\n",
    "        total[classes[np.argwhere(targets == 1)[0,0]]]+=1\n",
    "        probs = model.predict(inputs)\n",
    "        if np.argmax(probs) == np.argmax(targets):\n",
    "            n_correct+=1\n",
    "            correct[classes[np.argwhere(targets == 1)[0,0]]]+=1\n",
    "    percent_correct = (100.0 * n_correct / k)\n",
    "    if verbose:\n",
    "        print(\"Got an average of {}% {} way one-shot learning accuracy \\n\".format(percent_correct,N))\n",
    "    percentages = [(100.0*a)/b for a, b in zip(correct, total)]\n",
    "    return percentages, percent_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model on 10000 random 2 way one-shot learning tasks ... \n",
      "\n",
      "Got an average of 98.55% 2 way one-shot learning accuracy \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([98.48841774636827, 98.61394211169996], 98.55)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_oneshot(model, 2, 10000, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
