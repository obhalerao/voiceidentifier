{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.optimizers import *\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "\n",
    "NUM_CLASSES = 5\n",
    "\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadfiles():\n",
    "    imgs = []\n",
    "    labels = []\n",
    "    for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "        for filename in filenames:\n",
    "            file = os.path.join(dirname, filename)\n",
    "            ext = filename.split('.')[1]\n",
    "            if ext == 'csv':\n",
    "                label = int(filename.split('_')[1])\n",
    "                imgs.append(np.loadtxt(open(file, \"rb\"), delimiter=\",\", skiprows=1))\n",
    "                labels.append(label)\n",
    "    return np.array(imgs), np.array(labels)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs, labels = loadfiles()\n",
    "indices = np.array(list(range(imgs.shape[0])))\n",
    "np.random.shuffle(indices)\n",
    "test_imgs = imgs[indices[:1000]]\n",
    "test_labels = labels[indices[:1000]]\n",
    "train_imgs = imgs[indices[1000:]]\n",
    "train_labels = labels[indices[1000:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1.75853915e-04 1.17078140e-04 1.98359121e-04 ... 8.80602747e-05\n",
      "   1.09317116e-04 7.43524230e-04]\n",
      "  [2.98005296e-04 5.51570247e-06 1.05534218e-05 ... 2.36275118e-05\n",
      "   6.84056868e-05 7.08381296e-04]\n",
      "  [2.16972592e-04 4.01588932e-06 7.68376685e-06 ... 1.72027903e-05\n",
      "   4.98050213e-05 5.15760388e-04]\n",
      "  ...\n",
      "  [2.60817033e-04 6.30310024e-05 6.28012058e-05 ... 2.56397755e-07\n",
      "   6.71220448e-08 5.56050210e-08]\n",
      "  [1.25836377e-04 3.45871231e-05 4.93457665e-05 ... 5.17971955e-07\n",
      "   1.12310623e-07 6.33818829e-08]\n",
      "  [3.40120256e-04 1.26635496e-04 2.91933538e-05 ... 4.17313487e-07\n",
      "   1.54922247e-07 9.81054171e-08]]\n",
      "\n",
      " [[1.14557205e-03 1.61921920e-03 5.52529003e-04 ... 1.41617609e-04\n",
      "   5.51348639e-05 1.10962673e-03]\n",
      "  [1.92749838e-03 2.20328812e-02 1.44173326e-02 ... 3.89582448e-04\n",
      "   6.63545099e-04 4.08985885e-04]\n",
      "  [1.40337879e-03 1.60417669e-02 1.04970150e-02 ... 2.83648376e-04\n",
      "   4.83115931e-04 2.97775696e-04]\n",
      "  ...\n",
      "  [7.35013472e-08 4.39076295e-08 7.17849531e-08 ... 5.39676876e-05\n",
      "   7.38987219e-05 3.54672156e-05]\n",
      "  [4.66551370e-07 4.12111433e-07 2.36731537e-07 ... 1.14248694e-04\n",
      "   6.22440712e-05 2.22406961e-05]\n",
      "  [3.41297920e-07 6.15501676e-07 3.32450213e-07 ... 8.63535970e-05\n",
      "   8.75558108e-05 4.51396372e-05]]\n",
      "\n",
      " [[2.54505780e-04 4.36936389e-04 7.31673072e-06 ... 1.27911801e-04\n",
      "   1.41000637e-04 1.40992006e-05]\n",
      "  [1.07609958e-03 1.11271074e-04 7.27179722e-05 ... 2.83970527e-04\n",
      "   7.09255619e-05 1.79960844e-04]\n",
      "  [7.83489842e-04 8.10145793e-05 5.29447207e-05 ... 2.06754121e-04\n",
      "   5.16396976e-05 1.31026434e-04]\n",
      "  ...\n",
      "  [2.03431000e-06 3.03598517e-06 4.44655916e-06 ... 5.62301602e-06\n",
      "   6.02314276e-06 1.06459884e-05]\n",
      "  [1.94783570e-06 4.37338531e-06 2.10215990e-06 ... 3.08338190e-06\n",
      "   2.54182669e-06 1.04591481e-05]\n",
      "  [8.31334773e-07 9.17862906e-07 1.04783760e-06 ... 7.07100241e-07\n",
      "   8.55321673e-07 1.88800254e-06]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[2.56222580e-03 1.96907111e-03 5.00308676e-03 ... 5.16711327e-04\n",
      "   1.03114196e-03 2.80220225e-03]\n",
      "  [3.10394607e-05 2.49682944e-05 1.30089582e-04 ... 6.27186091e-05\n",
      "   8.40876382e-05 1.41273704e-04]\n",
      "  [2.25993044e-05 1.81789910e-05 9.47160152e-05 ... 4.56643538e-05\n",
      "   6.12227814e-05 1.02858983e-04]\n",
      "  ...\n",
      "  [1.20570066e-05 7.37893715e-05 1.41185228e-04 ... 5.36465450e-06\n",
      "   1.13022963e-06 1.82482253e-07]\n",
      "  [9.90046101e-06 4.90926723e-05 1.01936646e-04 ... 6.91752064e-07\n",
      "   4.30236270e-07 2.43884898e-07]\n",
      "  [2.14795800e-05 6.56689153e-05 1.16832038e-04 ... 1.05144045e-06\n",
      "   1.37096708e-06 3.40900073e-07]]\n",
      "\n",
      " [[4.94980952e-04 1.66253152e-03 1.08181871e-03 ... 1.09046896e-03\n",
      "   2.15487345e-03 1.81062997e-03]\n",
      "  [7.41145122e-05 8.16235934e-06 4.64660261e-05 ... 4.02559817e-05\n",
      "   1.09721252e-04 3.33835778e-04]\n",
      "  [5.39615176e-05 5.94287530e-06 3.38311220e-05 ... 2.93096964e-05\n",
      "   7.98861802e-05 2.43060160e-04]\n",
      "  ...\n",
      "  [1.32406024e-08 5.95576921e-08 3.63806265e-08 ... 1.32953678e-03\n",
      "   3.69766291e-04 5.07032441e-04]\n",
      "  [2.35943922e-08 5.44868257e-08 1.11814707e-07 ... 8.78823630e-04\n",
      "   1.27982756e-03 8.20566202e-04]\n",
      "  [2.03755466e-08 8.04160791e-08 6.72426594e-08 ... 1.15613011e-03\n",
      "   6.58547506e-04 1.03451603e-03]]\n",
      "\n",
      " [[3.42513435e-03 2.99407431e-04 2.22647895e-06 ... 1.04584396e-04\n",
      "   2.63570837e-04 1.94255030e-04]\n",
      "  [3.78886308e-03 1.25494207e-05 4.02703416e-04 ... 3.02750304e-05\n",
      "   1.29124746e-04 5.32291888e-05]\n",
      "  [2.75860680e-03 9.13702024e-06 2.93201534e-04 ... 2.20427355e-05\n",
      "   9.40135433e-05 3.87552682e-05]\n",
      "  ...\n",
      "  [1.87186799e-06 2.81363725e-07 1.71422144e-07 ... 2.15606565e-06\n",
      "   1.97266013e-06 2.33404580e-06]\n",
      "  [8.23200082e-07 3.53484381e-07 1.99923875e-07 ... 3.38356108e-06\n",
      "   1.54035854e-06 7.76820059e-07]\n",
      "  [2.04623348e-07 1.05144174e-07 8.98472194e-08 ... 3.30219109e-06\n",
      "   4.16164141e-07 5.20333913e-07]]]\n"
     ]
    }
   ],
   "source": [
    "print(test_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_arr = [[] for i in range(NUM_CLASSES)]\n",
    "for idx, i in enumerate(train_labels):\n",
    "    train_arr[i].append(idx)\n",
    "test_arr = [[] for i in range(NUM_CLASSES)]\n",
    "for idx, i in enumerate(test_labels):\n",
    "    test_arr[i].append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size, s=\"train\"):\n",
    "    global NUM_CLASSES, train_imgs, test_imgs, train_labels, test_labels, train_arr, test_arr\n",
    "    if s == \"train\":\n",
    "        imgs = train_imgs\n",
    "        labels = train_labels\n",
    "        arr = train_arr\n",
    "    else:\n",
    "        imgs = test_imgs\n",
    "        labels = test_labels\n",
    "        arr = test_arr\n",
    "        \n",
    "    \n",
    "    n_classes = NUM_CLASSES\n",
    "    n_examples, w, h = imgs.shape\n",
    "    pairs = [np.zeros((batch_size, w, h, 1)) for i in range(2)]\n",
    "    targets = np.zeros((batch_size,))\n",
    "    \n",
    "    targets[batch_size//2:] = 1\n",
    "    \n",
    "    categories = np.random.randint(0, n_classes, batch_size)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        category = categories[i]\n",
    "        idx_1 = random.choice(arr[category])\n",
    "        pairs[0][i,:,:,:] = imgs[idx_1].reshape(w, h, 1)\n",
    "        if i >= batch_size // 2:\n",
    "            category_2 = category  \n",
    "        else: \n",
    "            category_2 = (category + np.random.randint(1,n_classes)) % n_classes\n",
    "        idx_2 = random.choice(arr[category_2])\n",
    "        \n",
    "        pairs[1][i,:,:,:] = imgs[idx_2].reshape(w, h, 1)\n",
    "    \n",
    "    return pairs, targets\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(batch_size, s=\"train\"):\n",
    "    while True:\n",
    "        pairs, targets = get_batch(batch_size, s)\n",
    "        yield (pairs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_siamese_model(input_shape):\n",
    "    \"\"\"\n",
    "        Model architecture\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define the tensors for the two input images\n",
    "    left_input = Input(input_shape)\n",
    "    right_input = Input(input_shape)\n",
    "    \n",
    "    # Convolutional Neural Network\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (10,10), activation='relu', input_shape=input_shape,\n",
    "                   kernel_initializer='random_normal', kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(128, (7,7), activation='relu',\n",
    "                     kernel_initializer='random_normal',\n",
    "                     bias_initializer='random_normal', kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(128, (4,4), activation='relu', kernel_initializer='random_normal',\n",
    "                     bias_initializer='random_normal', kernel_regularizer=l2(2e-4)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(256, (4,4), activation='relu', kernel_initializer='random_normal',\n",
    "                     bias_initializer='random_normal', kernel_regularizer=l2(2e-4)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='sigmoid',\n",
    "                   kernel_regularizer=l2(1e-3),\n",
    "                   kernel_initializer='random_normal',bias_initializer='random_normal'))\n",
    "    \n",
    "    # Generate the encodings (feature vectors) for the two images\n",
    "    encoded_l = model(left_input)\n",
    "    encoded_r = model(right_input)\n",
    "    \n",
    "    # Add a customized layer to compute the absolute difference between the encodings\n",
    "    L1_layer = Lambda(lambda tensors:K.abs(tensors[0] - tensors[1]))\n",
    "    L1_distance = L1_layer([encoded_l, encoded_r])\n",
    "    \n",
    "    # Add a dense layer with a sigmoid unit to generate the similarity score\n",
    "    prediction = Dense(1,activation='sigmoid',bias_initializer='random_normal')(L1_distance)\n",
    "    \n",
    "    # Connect the inputs with the outputs\n",
    "    siamese_net = Model(inputs=[left_input,right_input],outputs=prediction)\n",
    "    \n",
    "    # return the model\n",
    "    return siamese_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "100/100 [==============================] - 11s 108ms/step - loss: 98.5652\n",
      "Epoch 2/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 84.6551\n",
      "Epoch 3/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 72.9703\n",
      "Epoch 4/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 63.0958\n",
      "Epoch 5/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 54.7406\n",
      "Epoch 6/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 47.6479\n",
      "Epoch 7/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 41.6171\n",
      "Epoch 8/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 36.4504\n",
      "Epoch 9/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 32.0333\n",
      "Epoch 10/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 28.2257\n",
      "Epoch 11/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 24.9606\n",
      "Epoch 12/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 22.1526\n",
      "Epoch 13/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 19.7055\n",
      "Epoch 14/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 17.6169\n",
      "Epoch 15/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 15.7875\n",
      "Epoch 16/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 14.2083\n",
      "Epoch 17/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 12.8340\n",
      "Epoch 18/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 11.6394\n",
      "Epoch 19/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 10.5929\n",
      "Epoch 20/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 9.6850\n",
      "Epoch 21/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 8.8846\n",
      "Epoch 22/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 8.1820\n",
      "Epoch 23/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 7.5579\n",
      "Epoch 24/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 7.0073\n",
      "Epoch 25/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 6.5143\n",
      "Epoch 26/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 6.0922\n",
      "Epoch 27/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 5.7022\n",
      "Epoch 28/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 5.3532\n",
      "Epoch 29/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 5.0372\n",
      "Epoch 30/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 4.7687\n",
      "Epoch 31/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 4.5117\n",
      "Epoch 32/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 4.2856\n",
      "Epoch 33/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 4.0653\n",
      "Epoch 34/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 3.8644\n",
      "Epoch 35/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 3.6988\n",
      "Epoch 36/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 3.5359\n",
      "Epoch 37/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 3.3862\n",
      "Epoch 38/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 3.2550\n",
      "Epoch 39/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 3.1177\n",
      "Epoch 40/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 3.0267\n",
      "Epoch 41/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 2.9150\n",
      "Epoch 42/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 2.8018\n",
      "Epoch 43/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 2.7007\n",
      "Epoch 44/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 2.6134\n",
      "Epoch 45/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 2.5302\n",
      "Epoch 46/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 2.4477\n",
      "Epoch 47/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 2.3654\n",
      "Epoch 48/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 2.2990\n",
      "Epoch 49/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 2.2289\n",
      "Epoch 50/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 2.1722\n",
      "Epoch 51/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 2.1142\n",
      "Epoch 52/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 2.0496\n",
      "Epoch 53/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 2.0124\n",
      "Epoch 54/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 1.9515\n",
      "Epoch 55/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 1.0336\n",
      "Epoch 84/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 1.0233\n",
      "Epoch 85/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 1.0161\n",
      "Epoch 86/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 1.0032\n",
      "Epoch 87/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.9739\n",
      "Epoch 88/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.9746\n",
      "Epoch 89/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.9456\n",
      "Epoch 90/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.9441\n",
      "Epoch 91/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.9428\n",
      "Epoch 92/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.9068\n",
      "Epoch 93/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.9146\n",
      "Epoch 94/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.8950\n",
      "Epoch 95/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 0.8847\n",
      "Epoch 96/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.8568\n",
      "Epoch 97/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.8633\n",
      "Epoch 98/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 0.8592\n",
      "Epoch 99/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.8375\n",
      "Epoch 100/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.8318\n",
      "Epoch 101/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.8198\n",
      "Epoch 102/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.8309\n",
      "Epoch 103/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.8041\n",
      "Epoch 104/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.7860\n",
      "Epoch 105/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 0.7878\n",
      "Epoch 106/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 0.7747\n",
      "Epoch 107/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 0.7583\n",
      "Epoch 108/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.7566\n",
      "Epoch 109/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.7581\n",
      "Epoch 110/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.7447\n",
      "Epoch 111/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.7415\n",
      "Epoch 112/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.7270\n",
      "Epoch 113/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.7241\n",
      "Epoch 114/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.7157\n",
      "Epoch 115/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.7184\n",
      "Epoch 116/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.7077\n",
      "Epoch 117/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.7039\n",
      "Epoch 118/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.6899\n",
      "Epoch 119/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.6758\n",
      "Epoch 120/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.6685\n",
      "Epoch 121/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.6756\n",
      "Epoch 122/300\n",
      "100/100 [==============================] - 6s 55ms/step - loss: 0.6697\n",
      "Epoch 123/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.6690\n",
      "Epoch 124/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.6634\n",
      "Epoch 125/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.6342\n",
      "Epoch 126/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.6526\n",
      "Epoch 127/300\n",
      "100/100 [==============================] - 6s 58ms/step - loss: 0.6358\n",
      "Epoch 128/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.6282\n",
      "Epoch 129/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.6195\n",
      "Epoch 130/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.6311\n",
      "Epoch 131/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.6057\n",
      "Epoch 132/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.6294\n",
      "Epoch 133/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.6163\n",
      "Epoch 134/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.6200\n",
      "Epoch 135/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.5987\n",
      "Epoch 136/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.6057\n",
      "Epoch 137/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.5803\n",
      "Epoch 138/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.5904\n",
      "Epoch 139/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.5929\n",
      "Epoch 140/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.5764\n",
      "Epoch 141/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.5781\n",
      "Epoch 142/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.5832\n",
      "Epoch 143/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.5767\n",
      "Epoch 144/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.5549\n",
      "Epoch 145/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.5751\n",
      "Epoch 146/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.5560\n",
      "Epoch 147/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.5480\n",
      "Epoch 148/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.5346\n",
      "Epoch 149/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.5373\n",
      "Epoch 150/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.5282\n",
      "Epoch 151/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.5413\n",
      "Epoch 152/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.5387\n",
      "Epoch 153/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.5402\n",
      "Epoch 154/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.5255\n",
      "Epoch 155/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.5291\n",
      "Epoch 156/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.5453\n",
      "Epoch 157/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.5098\n",
      "Epoch 158/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.5243\n",
      "Epoch 159/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.5059\n",
      "Epoch 160/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.4916\n",
      "Epoch 161/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.5043\n",
      "Epoch 162/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.5082\n",
      "Epoch 163/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.4864\n",
      "Epoch 164/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.4781\n",
      "Epoch 165/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.4899\n",
      "Epoch 166/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.4974\n",
      "Epoch 167/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.4896\n",
      "Epoch 168/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.4825\n",
      "Epoch 169/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.4877\n",
      "Epoch 170/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.4778\n",
      "Epoch 171/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.4779\n",
      "Epoch 172/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.4715\n",
      "Epoch 173/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.4658\n",
      "Epoch 174/300\n",
      "100/100 [==============================] - 6s 57ms/step - loss: 0.4765\n",
      "Epoch 175/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.4685\n",
      "Epoch 176/300\n",
      "100/100 [==============================] - 6s 56ms/step - loss: 0.4632\n",
      "Epoch 177/300\n",
      " 58/100 [================>.............] - ETA: 2s - loss: 0.4645"
     ]
    }
   ],
   "source": [
    "model = get_siamese_model(tuple(list(train_imgs[0].shape)+[1]))\n",
    "optimizer = Adam(lr = 0.00006)\n",
    "model.compile(loss=\"binary_crossentropy\",optimizer=optimizer)\n",
    "    \n",
    "model.fit(generate(32), epochs=300, steps_per_epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('epoch150-weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
